{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e75c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from typing import List\n",
    " \n",
    "import torch\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseOutputParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    " \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911ccb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d2a118484944abab02adf42f1782d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "MODEL_NAME = \"fine_tuned_model_kbase_all_data\"\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\"\n",
    ")\n",
    "model = model.eval()\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a70516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model genration config\n",
    "generation_config = model.generation_config\n",
    "generation_config.temperature = 0\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.repetition_penalty = 1.7\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    " \n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "chain = ConversationChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c281fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompt\n",
    "template = \"\"\"\n",
    "Current conversation:\n",
    "{history}\n",
    "Question: {input}\n",
    "Answer:\"\"\".strip()\n",
    " \n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1ef578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add memory to the model\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"history\", k=6, return_only_outputs=True\n",
    ")\n",
    " \n",
    "chain = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
    "def kbasebot(text_input):\n",
    "    res = chain.predict(input=text_input)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dea92b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCurrent conversation:\n",
      "\n",
      "Question: how to use KBase narrative?\n",
      "Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " To use KBase narrative, you can follow these steps:\n",
      "\n",
      "1. Go to the KBase website and sign in with your Google or Globus account.\n",
      "2. Click on the \"Narrative\" tab at the top of the page.\n",
      "3. From there, you can create a new narrative by clicking on the green button that says \"New Narrative\".\n",
      "4. Once you have created a new narrative, you will be taken to the narrative editor where you can start adding content such as text, images, videos, and more.\n",
      "5. You can also add collaborators to your narrative if you want them to contribute or view it.\n",
      "6. After making changes to your narrative, save it using the Save button located at the top right corner of the screen.\n",
      "7. Your saved narrative will then appear under the Narratives section on the left-hand side of the page.\n",
      "8. You can edit or delete this narrative later if needed.\n",
      "\n",
      "By following these steps, you should now know how to use KBase narrative for creating and managing scientific narratives.\n"
     ]
    }
   ],
   "source": [
    "print(kbasebot(\"how to use KBase narrative?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5471ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCurrent conversation:\n",
      "Human: how to use KBase narrative?\n",
      "AI:  To use KBase narrative, you can follow these steps:\n",
      "\n",
      "1. Go to the KBase website and sign in with your Google or Globus account.\n",
      "2. Click on the \"Narrative\" tab at the top of the page.\n",
      "3. From there, you can create a new narrative by clicking on the green button that says \"New Narrative\".\n",
      "4. Once you have created a new narrative, you will be taken to the narrative editor where you can start adding content such as text, images, videos, and more.\n",
      "5. You can also add collaborators to your narrative if you want them to contribute or view it.\n",
      "6. After making changes to your narrative, save it using the Save button located at the top right corner of the screen.\n",
      "7. Your saved narrative will then appear under the Narratives section on the left-hand side of the page.\n",
      "8. You can edit or delete this narrative later if needed.\n",
      "\n",
      "By following these steps, you should now know how to use KBase narrative for creating and managing scientific narratives.\n",
      "Question: can you explain more for step 3?\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Sure! In the context of KBase, when you click on the green button labeled \"New Narrative,\" you are brought to the narrative editor interface. This allows you to design and develop detailed stories or reports based on specific topics or research projects. The narrative editor provides an intuitive drag-and-drop user interface that enables you to easily arrange and organize different elements within your story, including text, visual materials, interactive features, and other media assets. By utilizing various tools available in the editor, you can quickly build engaging multimedia experiences that effectively communicate your ideas and findings to others.\n"
     ]
    }
   ],
   "source": [
    "print(kbasebot(\"can you explain more for step 3?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379dbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
